# Awesome-deep-multimodal-reasoning
Collect the awesome works evolved around reasoning models like O1/R1 in multimodal domain.

### Papers/Projects
#### Visual Understanding

- **[Image]** Visual Reinforcement Fine-Tuning [[paper]](https://arxiv.org/abs/2503.01785) [[code]](https://github.com/Liuziyu77/Visual-RFT) [[ViRFT]](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df)
- **[Image]** VLM-R1: A stable and generalizable R1-style Large Vision-Language Model [[code]](https://github.com/om-ai-lab/VLM-R1) [[VLM-R1 Data]](https://huggingface.co/datasets/omlab/VLM-R1) 
- **[Image]** Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement [[paper]](https://arxiv.org/abs/2503.06520) [[code]](https://github.com/dvlab-research/Seg-Zero) [ [refCOCOg_2k_840]](https://huggingface.co/datasets/Ricky06662/refCOCOg_2k_840)
- **[Video]** Video-R1: Towards Super Reasoning Ability in Video Understanding [[code]](https://github.com/tulerfeng/Video-R1) 
- **[Video]** Open R1 Video [[code]](https://github.com/Wang-Xiaodong1899/Open-R1-Video) [[open-r1-video-4k]](https://huggingface.co/datasets/Xiaodong/open-r1-video-4k)
- **[Omni]** MM-RLHF: The Next Step Forward in Multimodal LLM Alignment [[paper]](https://arxiv.org/abs/2502.10391) [[code]](https://github.com/Kwai-YuanQi/MM-RLHF) [[MM-RLHF Data]](https://huggingface.co/datasets/yifanzhang114/MM-RLHF) [[MM-RLHF-RewardBench]](https://huggingface.co/datasets/yifanzhang114/MM-RLHF-RewardBench)
- **[Omni]** R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning [[paper]](https://arxiv.org/abs/2503.05379) [[code]](https://github.com/HumanMLLM/R1-Omni)

#### Visual Reasoning

- R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [[paper]](https://arxiv.org/pdf/2503.10615) [[code]](https://github.com/Fancy-MLLM/R1-Onevision) [[R1-Onevision Data]](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision) 
- MMR1: Advancing the Frontiers of Multimodal Reasoning [[code]](https://github.com/LengSicong/MMR1) [[MMR1-Math-RL-Data-v0]](https://huggingface.co/datasets/MMR1/MMR1-Math-RL-Data-v0)
- LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [[paper]](https://arxiv.org/abs/2503.07536) [[code]](https://github.com/TideDra/lmm-r1) [[VerMulti]](https://huggingface.co/datasets/VLM-Reasoner/VerMulti) 
- R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[paper]](https://arxiv.org/abs/2503.05132) [code](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)
- R1-Vision: Let's first take a look at the image [[code]](https://github.com/yuyq96/R1-Vision/tree/main) [[R1-Vision Data]](https://huggingface.co/collections/yuyq96/r1-vision-67a6fb7898423dca453efa83)
- MM-EUREKA: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning  [[paper]](https://github.com/ModalMinds/MM-EUREKA/blob/main/MM_Eureka_paper.pdf) [[code]](https://github.com/ModalMinds/MM-EUREKA) [[MM-Eureka-Dataset]](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset/tree/main)
- Multimodal Open R1 [[code]](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal) [[multimodal-open-r1-8k-verified]](https://huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified) 
- VL-Thinking: An R1-Derived Visual Instruction Tuning Dataset for Thinkable LVLMs [[code]](https://github.com/UCSC-VLAA/VL-Thinking) [[VL-Thinking]](https://huggingface.co/datasets/UCSC-VLAA/VL-Thinking)
- R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3 [[code]](https://github.com/Deep-Agent/R1-V) [[R1V Training Dataset: CLEVR-70k-Counting]](https://huggingface.co/datasets/leonardPKU/clevr_cogen_a_train) [[R1V Training Dataset: CLEVR-70k-Complex]](https://huggingface.co/datasets/MMInstruction/Clevr_CoGenT_TrainA_70K_Complex) [[R1V Training Dataset: GEOQA-8k]](https://huggingface.co/datasets/leonardPKU/GEOQA_R1V_Train_8K) [[R1-Distilled Visual Reasoning Dataset]](https://huggingface.co/datasets/MMInstruction/Clevr_CoGenT_TrainA_R1)
- LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs [[paper]](https://arxiv.org/abs/2501.06186) [[code]](https://github.com/mbzuai-oryx/LlamaV-o1) [[VRC-Bench]](https://huggingface.co/datasets/omkarthawakar/VRC-Bench) 
- Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[paper]](https://arxiv.org/abs/2503.06749) [[code]](https://github.com/Osilly/Vision-R1/tree/main)

#### Robotics/Spatial

- Imagine while Reasoning in Space: Multimodal Visualization-of-Thought [[paper]](https://arxiv.org/abs/2501.07542)
- MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse [[code]](https://github.com/PzySeere/MetaSpatial)

#### Medical

- MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning [[paper]](https://arxiv.org/abs/2502.19634v1)
- HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs [[paper]](https://arxiv.org/abs/2412.18925) [[code]](https://github.com/FreedomIntelligence/HuatuoGPT-o1) [[medical-o1-reasoning-SFT]](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)

#### Benchmarks

- MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models [[paper]](https://arxiv.org/abs/2502.00698) [[code]](https://github.com/AceCHQ/MMIQ)
- MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency [[paper]](https://arxiv.org/abs/2502.09621) [[code]](https://github.com/CaraJ7/MME-CoT)
- ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models [[paper]](https://arxiv.org/pdf/2502.09696) [[code]](https://github.com/jonathan-roberts1/zerobench/)

### Datasets

- **[Training]** [LLaVA-R1-100k](https://www.modelscope.cn/datasets/modelscope/LLaVA-R1-100k) - LLaVA多模态Reasoning数据集
- **[Benchmarking]** [MMMU-Reasoning-R1-Distill-Validation](https://www.modelscope.cn/datasets/modelscope/MMMU-Reasoning-Distill-Validation) - MMMU-满血版R1蒸馏多模态Reasoning验证集

### Infra

- EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL [[code]](https://github.com/hiyouga/EasyR1)
- verl: Volcano Engine Reinforcement Learning for LLM [[code]](https://github.com/volcengine/verl)
- TRL - Transformer Reinforcement Learning [[code]](https://github.com/huggingface/trl)
- Align-Anything: Training All-modality Model with Feedback [[code]](https://github.com/PKU-Alignment/align-anything)
- R-Chain: A lightweight toolkit for distilling reasoning models [[code]](https://github.com/modelscope/r-chain)

### Related Collections

- [Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs)
- [modelscope/awesome-deep-reasoning](https://github.com/modelscope/awesome-deep-reasoning)
- [atfortes/Awesome-LLM-Reasoning](https://github.com/atfortes/Awesome-LLM-Reasoning)
- [hijkzzz/Awesome-LLM-Strawberry](https://github.com/hijkzzz/Awesome-LLM-Strawberry)

- [srush/awesome-o1](https://github.com/srush/awesome-o1)